version: '3.8'

services:
  emotalk-grpc:
    build:
      context: .
      dockerfile: Dockerfile.grpc
    container_name: emotalk-grpc-optimized
    ports:
      - "50051:50051"
    environment:
      - MODEL_PATH=/app/pretrain_model/EmoTalk.pth
      - DEVICE=cuda
      - NUM_WORKERS=2
      - QUEUE_SIZE=100
    volumes:
      - ./pretrain_model:/app/pretrain_model:ro
      - ./audio:/app/audio:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "python3", "-c", "from grpc_client_optimized import OptimizedEmoTalkClient; import sys; client = OptimizedEmoTalkClient('localhost:50051'); sys.exit(0 if client.health_check() else 1)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Optional: Prometheus metrics exporter
  # prometheus:
  #   image: prom/prometheus:latest
  #   ports:
  #     - "9090:9090"
  #   volumes:
  #     - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro

  # Optional: Grafana dashboard
  # grafana:
  #   image: grafana/grafana:latest
  #   ports:
  #     - "3000:3000"
  #   environment:
  #     - GF_SECURITY_ADMIN_PASSWORD=admin
